\documentclass{ETHExercise}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{float}
\usepackage{subfig}
\usepackage{adjustbox}
\usepackage[margin=2cm]{geometry}
\usepackage{fancyhdr}
\usepackage{capt-of}
\usepackage{tabularx}
\usepackage{bm}
\usepackage{amsmath}
\usepackage{datetime}
\usepackage{listings}


\pagestyle{fancy}
\fancyhf{}
\fancyfoot[L]{263-5902-00L W23 / Lab report}
\fancyfoot[C]{\thepage}
\fancyfoot[R]{Computer Science\\ETH ZÃ¼rich}
\renewcommand{\footrulewidth}{0.05pt}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrule}{\hrule width\textwidth height\footrulewidth \vskip 2pt}
\newcommand{\timestamp}{\ddmmyyyydate\today \,\,- \currenttime h}

\usepackage{mdframed}
\usepackage{etoolbox}

\newtoggle{showA}
\toggletrue{showA}

\newenvironment{answer}{
  \begin{mdframed}[linewidth=1pt,linecolor=black!30,backgroundcolor=white]\iftoggle{showA}{\textbf{A:}}{}}{
  \end{mdframed}
}

\renewcommand{\thesubfigure}{\roman{subfigure}}


\hypersetup{colorlinks,urlcolor=blue}
\lstset{
  backgroundcolor=\color{lightgray!20},
  basicstyle=\small\ttfamily,
  keywordstyle=\color{blue}\ttfamily,
  language=C,
  numbers=left,
  stepnumber=1,
  tabsize=4,
  showspaces=false,
  showstringspaces=false,
}
\title{Lab 2}
\author{Filippo Ficarra}
\begin{document}


\lectureheader{Prof. M. Pollefeyes}
{}
{\Large Computer Vision}{Fall 2023}
\begin{center}
    {\Huge Filippo Ficarra: Lab 2 - Feature extraction and Optical flow }\\
      \quad\newline
      fficarra@student.ethz.ch, 22--938--062.\\
      \quad\newline
      \timestamp
      \end{center}

\section{Introduction}
In this lab we aim to extract features from images, 
such as corners, using Harris detection and match descriptors 
between two different photos of the same thing.

\section{Harris corner detection}
The idea of Harris corner detection is to analyze the change
of intensity of pixels in a window. Using this idea we can 
basically identify three different regions:
\begin{itemize}
  \item flat: there is no change of intensity in all the directions
  \item edge: there is no change of intensity in the direction of the edge
  \item corner: large change of intensity in all the directions
\end{itemize}

To perform this analysis we define a window $\bm{W}$ and we define
the SSD error as:
\begin{center}
  $E(u, v) = \sum\limits_{(x, y) \in \bm{W}} \left[I(x+u, y+v) - I(x, y)\right]^2$
\end{center}
using Taylor approximation we can approximate the error with:
\begin{center}
  $E(\Delta x, \Delta v) \approx \left[\Delta x, \Delta y\right] M \begin{bmatrix}
    \Delta x\\
    \Delta y
  \end{bmatrix}$, $M = \sum \begin{bmatrix}
    I_x^2 & I_xI_y\\
    I_xI_y & I_y^2
  \end{bmatrix}$
\end{center}
Note that $I_y$ and $I_y$ are respectively the partial derivative of the image
with respect to x and y.

\subsection{Image derivatives}

Since images are discrete we need to compute some approximation of the derivative.
The approximation used in this case is the following:
\begin{center}
  $I_x = \frac{I(x+1, y)- I(x-1, y)}{2}$, $I_y = \frac{I(x, y+1)- I(x, y-1)}{2}$
\end{center}
We can exploit the convolution operations to compute these derivatives, 
using the following filters:
\begin{center}
  filter\textsubscript{x} = $\begin{bmatrix}
    \begin{bmatrix}
      -0.5 & 0 & 0.5
    \end{bmatrix}
  \end{bmatrix}
  $, filter\textsubscript{y} = $\begin{bmatrix}
    \begin{bmatrix}
      -0.5 
    \end{bmatrix},
    \begin{bmatrix}
     0 
    \end{bmatrix},
    \begin{bmatrix}
    0.5
    \end{bmatrix}
  \end{bmatrix}
  $
\end{center}
This convolutions have been done in python in the following way:
\begin{lstlisting}[language=Python, caption=Image gradients]
  filter_x = np.array([[1/2, 0, -1/2]])
  filter_y = np.array([[1/2],[0],[-1/2]])

  I_x = signal.convolve2d(img, filter_x, mode='same')
  I_y = signal.convolve2d(img, filter_y, mode='same')
\end{lstlisting}

\subsection{title}



\end{document}
